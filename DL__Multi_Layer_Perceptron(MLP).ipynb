{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "FbTY8y_y737f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "3frXM66L76g1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
      ],
      "metadata": {
        "id": "UujxNshy8Co_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling (standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "-qUOyGvp8HTX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)  #single hidden layer with 10 neurons. the data passes through the model 1000 times.\n",
        "mlp.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "JMmrJC4Z8P3Q",
        "outputId": "b7230e77-da8c-4bca-d2b9-d02aff86ca07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**hidden_layer_sizes=(10,)**: This representation uses a tuple with one element, which is 10. It indicates that there is a single hidden layer with 10 nodes (neurons).\n",
        "\n",
        "\n",
        "So, (10,) means a **single hidden layer with 10 neurons** in the MLP model.\n",
        "\n",
        "\n",
        "If there were more hidden layers, you would specify the number of neurons for each layer as separate elements in the tuple.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TlXPwM_WRIJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = mlp.predict(X_test)"
      ],
      "metadata": {
        "id": "A2_FtUD48Toi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJV8sa0w57A4",
        "outputId": "56b73f86-6428-4d0e-eab2-04efd46cf5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get information about the model\n",
        "print(\"Number of hidden layers:\", len(mlp.coefs_) - 1)\n",
        "print(\"Number of neurons in each hidden layer:\", [layer.shape[1] for layer in mlp.coefs_[:-1]])\n",
        "print(\"Number of output neurons (classes):\", mlp.coefs_[-1].shape[1])\n",
        "print(\"Training loss:\", mlp.loss_)\n",
        "print(\"Number of iterations to converge:\", mlp.n_iter_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiG28tVD4YOO",
        "outputId": "20466d9b-4df1-46ac-927b-1df3208e9f62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of hidden layers: 1\n",
            "Number of neurons in each hidden layer: [10]\n",
            "Number of output neurons (classes): 3\n",
            "Training loss: 0.09253452971808651\n",
            "Number of iterations to converge: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we access the coefs_ attribute of the trained MLPClassifier model to get information about the weights and biases. The coefs_ attribute is a list of arrays, where each array represents the weights connecting the layers in the MLP. The length of the coefs_ list corresponds to the number of layers (including the input and output layers). The number of hidden layers is given by len(mlp.coefs_) - 1, and the number of neurons in each hidden layer can be obtained from the shape of the weight arrays using a list comprehension. The number of output neurons (classes) is given by the shape of the weights connecting the last hidden layer to the output layer.\n",
        "\n",
        "The loss_ attribute gives you the final training loss value, and the n_iter_ attribute provides the number of iterations taken for the optimization process to converge.\n",
        "\n",
        "While this doesn't give a comprehensive summary like some deep learning frameworks, it provides some useful information about the architecture and training process of the MLP."
      ],
      "metadata": {
        "id": "EY1Smmkg4rsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(np.prod(layer.shape) for layer in mlp.coefs_) + len(mlp.intercepts_)\n",
        "print(\"Total learnable parameters:\", total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzJyWenH49cQ",
        "outputId": "74329bc1-17ff-4522-db87-b0eef5c2d4ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total learnable parameters: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, after initializing and fitting the MLPClassifier model with X_train and y_train, we calculate the total number of learnable parameters.\n",
        "\n",
        "We use a list comprehension to calculate the number of parameters in each layer by using np.prod(layer.shape) which gives the total number of elements in each weight matrix. Then, we sum up the number of parameters for all layers and add the number of bias terms to get the total number of learnable parameters.\n",
        "\n",
        "Keep in mind that this calculation assumes that X_train is a 2D array representing the input data, and y_train is a 1D array representing the target labels. The number of features in X_train should match the number of elements in y_train. Also, the hidden_layer_sizes argument in the MLPClassifier specifies the number of neurons in the single hidden layer; in this example, it's set to (10,). You can adjust this parameter based on your specific problem and dataset.\n"
      ],
      "metadata": {
        "id": "sr5qYnlh63_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The learnable parameters in a neural network"
      ],
      "metadata": {
        "id": "PUraot0985h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a multi-layer perceptron:\n",
        "\n",
        "The multi-layer perceptron can have one or more hidden layers, each with its own set of weights and biases. Suppose the input has n features, there are m hidden neurons in each hidden layer, and the output layer has p neurons. Then, the total number of learnable parameters in a multi-layer perceptron is given by:\n"
      ],
      "metadata": {
        "id": "vzX9TqkC9p4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Total parameters = Number of parameters in input layer + Number of parameters in hidden layers + Number of parameters in output layer**\n",
        "\n",
        "**Number of parameters in input layer = Number of input features (n) * Number of hidden neurons in the first hidden layer (m)**\n",
        "\n",
        "**Number of parameters in hidden layers = (Number of hidden neurons (m) * Number of hidden neurons (m)) * (Number of hidden layers - 1)**\n",
        "\n",
        "**Number of parameters in output layer = Number of hidden neurons in the last hidden layer (m) * Number of output neurons (p)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fBryht2287og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let's consider a multi-layer perceptron with 3 input features, 2 hidden layers with 4 neurons each, and 1 output neuron:\n",
        "\n",
        "Number of parameters in input layer = 3 (input features) * 4 (hidden neurons in the first hidden layer) = 12\n",
        "\n",
        "Number of parameters in hidden layers = (4 (hidden neurons) * 4 (hidden neurons)) * (2 - 1) = 16\n",
        "\n",
        "Number of parameters in output layer = 4 (hidden neurons in the last hidden layer) * 1 (output neuron) = 4\n",
        "\n",
        "Total parameters = 12 (input layer) + 16 (hidden layers) + 4 (output layer) = 32"
      ],
      "metadata": {
        "id": "4SNGd50s9yCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to note that the above equations assume that the activation function used in all neurons is the same. Different activation functions or special architectures may introduce additional parameters."
      ],
      "metadata": {
        "id": "j2VygLvz9efA"
      }
    }
  ]
}